---
title: "Milestone Report"
author: "Yasser Gonzalez"
date: "March 20, 2016"
output:
  html_document:
    keep_md: true
---

```{r include = FALSE}
library("knitr")
library("dplyr")
library("tidyr")
library("R.utils")
library("stringi")
library("ggplot2")
library("RWeka")

corpus_path <- function(name, lang) {
    stopifnot(name %in% c("blogs", "news", "twitter"))
    stopifnot(lang %in% c("de_DE", "en_US", "fi_FI", "ru_RU"))

    data_dir <- "../data"
    lang_dir <- file.path(data_dir, lang)
    text_name <- paste0(lang, ".", name, ".txt")
    text_path <- file.path(lang_dir, text_name)

    text_path
}
```

## Introduction

This report presents an exploratory data analysis of the English documents
that will be used to train a model for predicting the next word in a given
sequence of words. The documents are part of a larger corpus  called
[HC Corpora](http://www.corpora.heliohost.org) and were collected from
blogs, news and Twitter.

```{r include = FALSE}
lang <- "en_US"
corpus_names <- c("blogs", "news", "twitter")
```

## Basic Statistics

This section presents some basic statistics about the training documents.

The table below reports the size of the files containing the documents
and the number of lines.

```{r basic_stats, include = FALSE, warning = FALSE, cache = TRUE}
format_size <- function(file_path) {
    size_in_bytes <- file.size(file_path)
    size_in_mb <- size_in_bytes / 10 ^ 6
    formatC(size_in_mb, format = "f", digits = 2, big.mark = ",")
}

format_lines <- function(file_path) {
    lines <- countLines(file_path)
    prettyNum(lines, big.mark = ",")
}

corpus_results <- function(corpus_name) {
    list(corpus = rep(corpus_name, 2),
         name = c("size", "lines"),
         value = c(format_size(corpus_path(corpus_name, lang)),
                   format_lines(corpus_path(corpus_name, lang))))
}

basic_stats <- do.call(rbind, lapply(lapply(corpus_names, corpus_results), as_data_frame))
```

```{r echo = FALSE}
kable(basic_stats %>%
    spread(name, value) %>%
    select(Corpus = corpus,
           `File Size (mb)` = size,
           `Number of Lines` = lines))
```

The following table summarizes the distribution of the number of words
per line in each file.

```{r words_per_line, include = FALSE, cache = TRUE}
corpus_results <- function(corpus_name) {
    lines <- readLines(corpus_path(corpus_name, lang), encoding = "UTF-8")
    word_count <- stri_count_words(lines)
    word_count_summary <- summary(word_count)

    results <- list(Corpus = corpus_name)
    results <- c(results, lapply(word_count_summary, function(x) x))
    results
}

words_per_line <- do.call(rbind, lapply(lapply(corpus_names, corpus_results), as_data_frame))
```

```{r echo = FALSE}
kable(words_per_line)
```

## N-Gram Frequency

This section illustrates the frequency of 1-grams, 2-grams and 3-grams in the
documents. Each plot presents the 10 most frequent $n$-grams for each
type of document along with their frequencies. The frequencies were
estimated from a random sample containing 5% of the lines in each file.

```{r ngram_data, cache = TRUE, include = FALSE}
top10_ngrams <- function(corpus_name, ngram_size, sample_rate) {
    lines <- readLines(corpus_path(corpus_name, lang), encoding = "UTF-8")
    lines <- sample(lines, round(sample_rate * length(lines)))
    lines <- stri_replace_all(lines, " ", regex = "[^a-zA-Z’']")
    lines <- stri_replace_all(lines, "'", fixed = "’")
    lines <- stri_replace_all(lines, " ", regex = "\\s+")
    lines <- stri_trans_tolower(lines)

    control <- Weka_control(min = ngram_size, max = ngram_size, delimiters = " ")
    ngrams <- data.frame(table(NGramTokenizer(lines, control)))
    names(ngrams) <- c("ngram", "freq")

    top10_ngrams <- ngrams %>%
        mutate(freq = 100 * freq / sum(freq)) %>%
        top_n(10, freq)

    top10_ngrams
}

ngram_data <- NULL
for (ngram_size in c(1, 2, 3)) {
    for (corpus_name in corpus_names) {
        corpus_data <- top10_ngrams(corpus_name, ngram_size, 0.001)
        corpus_data$ngram_size <- ngram_size
        corpus_data$corpus_name <- corpus_name
        ngram_data <- rbind(ngram_data, corpus_data)
    }
}
```

```{r include = FALSE}
plot_ngram_data <- function(n) {
    ggplot(filter(ngram_data, ngram_size == n),
           aes(x = reorder(ngram, freq), y = freq)) +
        facet_grid(~ corpus_name, scales = "free_y") +
        geom_bar(stat = "identity") +
        coord_flip() +
        ylab("Frequency (%)") +
        xlab(sprintf("%d-Gram", n))
}
```

```{r top10_1grams, echo = FALSE, fig.path = "figures/"}
plot_ngram_data(1)
```

```{r top10_2grams, echo = FALSE, fig.path = "figures/"}
plot_ngram_data(2)
```

```{r top10_3grams, echo = FALSE, fig.path = "figures/"}
plot_ngram_data(3)
```

## Conclusions & Outlook

As expected, the documents obtained from news and blogs are longer and have
a larger number of words than the Twitter documents. The analysis also revealed
some characteristics of the language used in the documents. For example, the texts
from blogs and Twitter frequently use personal pronouns -- such as 'I' and 'you' --
while the news documents use a more impersonal language.  The model for predicting
the next word in a sequence of words will be based on tables of $n$-gram
frequencies built from the training documents.
